{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "from scipy.spatial import distance_matrix\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LMDBDataset_cid:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "        assert os.path.isfile(self.db_path), \"{} not found\".format(self.db_path)\n",
    "        env = self.connect_db(self.db_path)\n",
    "        with env.begin() as txn:\n",
    "            self._keys = list(txn.cursor().iternext(values=False))\n",
    "\n",
    "    def connect_db(self, lmdb_path, save_to_self=False):\n",
    "        env = lmdb.open(\n",
    "            lmdb_path,\n",
    "            subdir=False,\n",
    "            readonly=True,\n",
    "            lock=False,\n",
    "            readahead=False,\n",
    "            meminit=False,\n",
    "            max_readers=256,\n",
    "        )\n",
    "        if not save_to_self:\n",
    "            return env\n",
    "        else:\n",
    "            self.env = env\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._keys)\n",
    "\n",
    "    @lru_cache(maxsize=16)\n",
    "    def __getitem__(self, cid):\n",
    "        if not hasattr(self, \"env\"):\n",
    "            self.connect_db(self.db_path, save_to_self=True)\n",
    "        datapoint_pickled = self.env.begin().get(cid.encode())\n",
    "        data = pickle.loads(datapoint_pickled)\n",
    "        return data\n",
    "\n",
    "class D3Dataset_cid(Dataset):\n",
    "    def __init__(self, path, max_atoms=256):\n",
    "        self.lmdb_dataset = LMDBDataset_cid(path)\n",
    "\n",
    "        self.max_atoms = max_atoms\n",
    "        ## the following is the default setting of uni-mol's pretrained weights\n",
    "        self.remove_hydrogen = True\n",
    "        self.remove_polar_hydrogen = False\n",
    "        self.normalize_coords = True\n",
    "        self.add_special_token = True\n",
    "        self.__max_atoms = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lmdb_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the cid from self.lmdb_dataset._keys[index]\n",
    "        cid = self.lmdb_dataset._keys[index].decode('utf-8')  # Decode bytes to string if necessary\n",
    "\n",
    "        # Retrieve the data using cid\n",
    "        data = self.lmdb_dataset[cid]\n",
    "        smiles = data['smiles']\n",
    "        description = data['description']\n",
    "        enriched_description = data['enriched_description']\n",
    "        ## deal with 3d coordinates\n",
    "        atoms_orig = np.array(data['atoms'])\n",
    "        atoms = atoms_orig.copy()\n",
    "        coordinates = data['coordinates']\n",
    "\n",
    "        return atoms, coordinates, smiles, description, enriched_description, cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/3d-pubchem.lmdb'\n",
    "lmdb_dataset = LMDBDataset_cid(path)\n",
    "cid = lmdb_dataset._keys[2].decode('utf-8')\n",
    "data = lmdb_dataset[cid]\n",
    "txt = data['enriched_description']\n",
    "sml = data['smiles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Encoders: Sci-Bert & MolT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\namjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertLMHeadModel\n",
    "\n",
    "def init_tokenizer():\n",
    "    bert_name = 'allenai/scibert_scivocab_uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
    "    tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
    "    return tokenizer\n",
    "\n",
    "# Import Sci-bert for text encoding\n",
    "tokenizer = init_tokenizer()\n",
    "encoder_config = BertConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertLMHeadModel.from_pretrained('allenai/scibert_scivocab_uncased', config=encoder_config)\n",
    "inputs = tokenizer(txt, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "cls_token = outputs[0][:,0,:]\n",
    "\n",
    "#############################################################################################################################################\n",
    "\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "text_encoder = T5ForConditionalGeneration.from_pretrained('laituan245/molt5-large-caption2smiles')\n",
    "text_tokenizer = T5Tokenizer.from_pretrained(\"laituan245/molt5-large-caption2smiles\", model_max_length=512)\n",
    "\n",
    "@torch.no_grad()\n",
    "def molT5_encoder(descriptions, molt5, molt5_tokenizer, description_length, device):\n",
    "    tokenized = molt5_tokenizer(descriptions, padding='max_length', truncation=True, max_length=description_length, return_tensors=\"pt\").to(device)\n",
    "    encoder_outputs = molt5.encoder(input_ids=tokenized.input_ids, attention_mask=tokenized.attention_mask, return_dict=True).last_hidden_state\n",
    "    return encoder_outputs, tokenized.attention_mask\n",
    "\n",
    "biot5_embed, pad_mask = molT5_encoder(txt, text_encoder, text_tokenizer, 256, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING PRETRAINED MODEL.. C:/Users/namjo/Downloads/Pretrain-20250121T212421Z-001/Pretrain/checkpoint_autoencoder.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namjo\\AppData\\Local\\Temp\\ipykernel_29456\\1706721132.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.vae, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder _IncompatibleKeys(missing_keys=[], unexpected_keys=['text_encoder2.embeddings.position_ids'])\n",
      "AE #parameters: 127993920, #trainable: 0\n"
     ]
    }
   ],
   "source": [
    "from utils import molT5_encoder, AE_SMILES_encoder, regexTokenizer\n",
    "from train_autoencoder import ldmol_autoencoder\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# Load pretrained encoder parameter \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--vae\", type=str, default=\"C:/Users/namjo/Downloads/Pretrain-20250121T212421Z-001/Pretrain/checkpoint_autoencoder.ckpt\")  # Choice doesn't affect training\n",
    "args = parser.parse_args([])\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Load pretrained encoder \n",
    "ae_config = {\n",
    "        'bert_config_decoder': './config_decoder.json',\n",
    "        'bert_config_encoder': './config_encoder.json',\n",
    "        'embed_dim': 256,\n",
    "    }\n",
    "\n",
    "tokenizer = regexTokenizer(vocab_path='./vocab_bpe_300_sc.txt', max_len=127) #newtkn\n",
    "ae_model = ldmol_autoencoder(config=ae_config, no_train=True, tokenizer=tokenizer, use_linear=True)\n",
    "\n",
    "if args.vae:\n",
    "    print('LOADING PRETRAINED MODEL..', args.vae)\n",
    "    checkpoint = torch.load(args.vae, map_location='cpu')\n",
    "    try:\n",
    "        state_dict = checkpoint['model']\n",
    "    except:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    msg = ae_model.load_state_dict(state_dict, strict=False)\n",
    "    print('autoencoder', msg)\n",
    "for param in ae_model.parameters():\n",
    "    param.requires_grad = False\n",
    "del ae_model.text_encoder\n",
    "ae_model = ae_model.to(device)\n",
    "ae_model.eval()\n",
    "\n",
    "print(f'AE #parameters: {sum(p.numel() for p in ae_model.parameters())}, #trainable: {sum(p.numel() for p in ae_model.parameters() if p.requires_grad)}')\n",
    "\n",
    "# Encode the Input SMILES representation \n",
    "@torch.no_grad()\n",
    "def AE_SMILES_encoder(sm, ae_model):\n",
    "    if sm[0][:5] == \"[CLS]\":    sm = [s[5:] for s in sm]\n",
    "    text_input = ae_model.tokenizer(sm).to(ae_model.device)\n",
    "    text_input_ids = text_input\n",
    "    text_attention_mask = torch.where(text_input_ids == 0, 0, 1).to(text_input.device)\n",
    "    if hasattr(ae_model.text_encoder2, 'bert'):\n",
    "        output = ae_model.text_encoder2.bert(text_input_ids, attention_mask=text_attention_mask, return_dict=True, mode='text').last_hidden_state\n",
    "    else:\n",
    "        output = ae_model.text_encoder2(text_input_ids, attention_mask=text_attention_mask, return_dict=True).last_hidden_state\n",
    "\n",
    "    if hasattr(ae_model, 'encode_prefix'):\n",
    "        output = ae_model.encode_prefix(output)\n",
    "        if ae_model.output_dim*2 == output.size(-1):\n",
    "            mean, logvar = torch.chunk(output, 2, dim=-1)\n",
    "            logvar = torch.clamp(logvar, -30.0, 20.0)\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            output = mean + std * torch.randn_like(mean)\n",
    "    return output\n",
    "\n",
    "sml_rep = AE_SMILES_encoder(sml, ae_model) # [1, 127, 64] # 127 is length of Tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "\n",
    "from unicore.data import Dictionary\n",
    "from model.unimol_simple import SimpleUniMolModel\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "SimpleUniMolModel.add_args(parser) \n",
    "args = parser.parse_args([])\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "def init_unimol_mol_encoder(args):\n",
    "    dictionary = Dictionary.load('unimol_dict_mol.txt')\n",
    "    dictionary.add_symbol(\"[MASK]\", is_special=True)\n",
    "    unimol_model = SimpleUniMolModel(args, dictionary)\n",
    "\n",
    "    ckpt = torch.load('mol_pre_no_h_220816.pt', map_location=torch.device('cpu'))['model']\n",
    "    missing_keys, unexpected_keys = unimol_model.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "    ln_graph = LayerNorm(unimol_model.num_features)\n",
    "    return unimol_model, ln_graph, dictionary\n",
    "\n",
    "# conf_encoder takes [src_token, src_distance, src_edge_type] as input\n",
    "conf_encoder, ln_conf, dictionary_mol = init_unimol_mol_encoder(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "dictionary = Dictionary.load('C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/unimol_dict_mol.txt')\n",
    "bos = dictionary.bos()\n",
    "eos = dictionary.eos()\n",
    "num_types = len(dictionary)\n",
    "\n",
    "path = 'C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/3d-pubchem.lmdb'\n",
    "\n",
    "lmdb_dataset = LMDBDataset_cid(path)\n",
    "cid = lmdb_dataset._keys[5].decode('utf-8')\n",
    "data = lmdb_dataset[cid]\n",
    "smils = data['smiles']\n",
    "atoms_orig = np.array(data['atoms'])\n",
    "atoms = atoms_orig.copy()\n",
    "coordinate_set = data['coordinates']\n",
    "txt = data['enriched_description']\n",
    "coordinates = random.sample(coordinate_set, 1)[0].astype(np.float32)\n",
    "\n",
    "mask_hydrogen = atoms != \"H\" # [True, True, True, True, .... , False]\n",
    "atoms = atoms[mask_hydrogen]\n",
    "coordinates = coordinates[mask_hydrogen]\n",
    "coordinates = coordinates - coordinates.mean(axis=0)\n",
    "atom_vec = torch.from_numpy(dictionary.vec_index(atoms)).long()\n",
    "\n",
    "atom_vec = torch.cat([torch.LongTensor([bos]), atom_vec, torch.LongTensor([eos])])\n",
    "coordinates = np.concatenate([np.zeros((1, 3)), coordinates, np.zeros((1, 3))], axis=0)\n",
    "\n",
    "edge_type = atom_vec.view(-1, 1) * num_types + atom_vec.view(1, -1)\n",
    "dist = distance_matrix(coordinates, coordinates).astype(np.float32)\n",
    "\n",
    "coordinates, dist = torch.from_numpy(coordinates), torch.from_numpy(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = conf_encoder(atom_vec.unsqueeze(0), dist.unsqueeze(0), edge_type.unsqueeze(0))\n",
    "from unicore.data import data_utils\n",
    "\n",
    "def collate_tokens_coords(\n",
    "    values,\n",
    "    pad_idx,\n",
    "    left_pad=False,\n",
    "    pad_to_length=None,\n",
    "    pad_to_multiple=1,\n",
    "):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values)\n",
    "    size = size if pad_to_length is None else max(size, pad_to_length)\n",
    "    if pad_to_multiple != 1 and size % pad_to_multiple != 0:\n",
    "        size = int(((size - 0.1) // pad_to_multiple + 1) * pad_to_multiple)\n",
    "    res = values[0].new(len(values), size, 3).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v) :, :] if left_pad else res[i][: len(v), :])\n",
    "    return res\n",
    "\n",
    "padded_coordinates = collate_tokens_coords(coordinates.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8) \n",
    "padded_edge_type = data_utils.collate_tokens_2d(edge_type.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8)\n",
    "padded_dist = data_utils.collate_tokens_2d(dist.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8)\n",
    "padded_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/3d-pubchem.lmdb'\n",
    "lmdb_dataset = LMDBDataset_cid(path)\n",
    "cid = lmdb_dataset._keys[0].decode('utf-8')\n",
    "cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.unimol_dataset import D3Dataset, D3Dataset_Pro\n",
    "target_path = 'C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/3d-pubchem.lmdb'\n",
    "dictionary = Dictionary.load('C:/Users/namjo/OneDrive/문서/GitHub/GeomCLIP/unimol_dict_mol.txt')\n",
    "d3_dataset = D3Dataset(target_path, dictionary, max_atoms=256)  # Number of molecules : 301658\n",
    "cid = lmdb_dataset._keys[1234].decode('utf-8')\n",
    "atom_vec, coordinates, edge_type, dist, smiles = d3_dataset[cid]\n",
    "\n",
    "def collate_tokens_coords(\n",
    "    values,\n",
    "    pad_idx,\n",
    "    left_pad=False,\n",
    "    pad_to_length=None,\n",
    "    pad_to_multiple=1,\n",
    "):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values)\n",
    "    size = size if pad_to_length is None else max(size, pad_to_length)\n",
    "    if pad_to_multiple != 1 and size % pad_to_multiple != 0:\n",
    "        size = int(((size - 0.1) // pad_to_multiple + 1) * pad_to_multiple)\n",
    "    res = values[0].new(len(values), size, 3).fill_(pad_idx)\n",
    "\n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v) :, :] if left_pad else res[i][: len(v), :])\n",
    "    return res\n",
    "\n",
    "from unicore.data import data_utils\n",
    "padded_atom_vec = data_utils.collate_tokens(atom_vec.unsqueeze(0), pad_idx=0, left_pad=False, pad_to_multiple=8) # shape = [batch_size, max_atoms]\n",
    "padded_edge_type = data_utils.collate_tokens_2d(edge_type.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8) # shape = [batch_size, max_atoms, max_atoms]\n",
    "padded_dist = data_utils.collate_tokens_2d(dist.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8) # shape = [batch_size, max_atoms, max_atoms]\n",
    "padded_coordinates = collate_tokens_coords(coordinates.unsqueeze(0), 0, left_pad=False, pad_to_multiple=8) # shape = [batch_size, max_atoms, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1, 1, 1],\n",
    "                   [2, 2, 2]])        # shape (2, 3)\n",
    "\n",
    "t2 = torch.tensor([[3, 3, 3]])        # shape (1, 3)\n",
    "\n",
    "t3 = torch.tensor([[4, 4, 4],\n",
    "                   [5, 5, 5],\n",
    "                   [6, 6, 6]])        # shape (3, 3)\n",
    "\n",
    "values = [t1, t2, t3]\n",
    "\n",
    "res_multiple = collate_tokens_coords(\n",
    "    values=[t1, t2, t3],\n",
    "    pad_idx=0,\n",
    "    left_pad=False,\n",
    "    pad_to_length=None,\n",
    "    pad_to_multiple=8\n",
    ")\n",
    "print(\"Result shape (multiple of 8):\", res_multiple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "# Step 1: Collect data for a random batch of CIDs\n",
    "random_cids = random.sample(lmdb_dataset._keys, 56)  # Choose 56 random keys\n",
    "random_cids = [cid.decode(\"utf-8\") for cid in random_cids]  # Decode the keys\n",
    "\n",
    "batch = [d3_dataset[cid] for cid in random_cids]  # Retrieve tuples\n",
    "atom_vecs, coordinates_list, edge_types, dists, smiles_list = zip(*batch)  # Unpack tuples\n",
    "\n",
    "# Step 2: Use collate_tokens_coords to unify coordinates\n",
    "pad_idx = 0  # Padding value for empty coordinates\n",
    "unified_coordinates = collate_tokens_coords(\n",
    "    values=coordinates_list,  # List of tensors\n",
    "    pad_idx=pad_idx,          # Padding value\n",
    "    left_pad=False,           # Right padding\n",
    "    pad_to_length=None,       # No minimum length specified\n",
    "    pad_to_multiple=512         # No rounding to multiples\n",
    ")\n",
    "\n",
    "# Step 3: Output the results\n",
    "print(\"Unified Coordinates Shape:\", unified_coordinates.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3Dataset_cid(Dataset):\n",
    "    def __init__(self, path, dictionary, max_atoms=256):\n",
    "        self.dictionary = dictionary\n",
    "        self.num_types = len(dictionary)\n",
    "        self.bos = dictionary.bos()\n",
    "        self.eos = dictionary.eos()\n",
    "\n",
    "        self.lmdb_dataset = LMDBDataset_cid(path)\n",
    "\n",
    "        self.max_atoms = max_atoms\n",
    "        ## the following is the default setting of uni-mol's pretrained weights\n",
    "        self.remove_hydrogen = True\n",
    "        self.remove_polar_hydrogen = False\n",
    "        self.normalize_coords = True\n",
    "        self.add_special_token = True\n",
    "        self.__max_atoms = 512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lmdb_dataset)\n",
    "\n",
    "    def __getitem__(self, cid):\n",
    "        data = self.lmdb_dataset[cid]\n",
    "        smiles = data['smiles']\n",
    "        description = data['description']\n",
    "        enriched_description = data['enriched_description']\n",
    "        ## deal with 3d coordinates\n",
    "        atoms_orig = np.array(data['atoms'])\n",
    "        atoms = atoms_orig.copy()\n",
    "        coordinate_set = data['coordinates']\n",
    "        coordinates = random.sample(coordinate_set, 1)[0].astype(np.float32)\n",
    "        assert len(atoms) == len(coordinates) and len(atoms) > 0\n",
    "        assert coordinates.shape[1] == 3\n",
    "\n",
    "        ## deal with the hydrogen\n",
    "        if self.remove_hydrogen:\n",
    "            mask_hydrogen = atoms != \"H\"\n",
    "            if sum(mask_hydrogen) > 0:\n",
    "                atoms = atoms[mask_hydrogen]\n",
    "                coordinates = coordinates[mask_hydrogen]\n",
    "\n",
    "        if not self.remove_hydrogen and self.remove_polar_hydrogen:\n",
    "            end_idx = 0\n",
    "            for i, atom in enumerate(atoms[::-1]):\n",
    "                if atom != \"H\":\n",
    "                    break\n",
    "                else:\n",
    "                    end_idx = i + 1\n",
    "            if end_idx != 0:\n",
    "                atoms = atoms[:-end_idx]\n",
    "                coordinates = coordinates[:-end_idx]\n",
    "\n",
    "        ## deal with cropping\n",
    "        if len(atoms) > self.max_atoms:\n",
    "            index = np.random.permutation(len(atoms))[:self.max_atoms]\n",
    "            atoms = atoms[index]\n",
    "            coordinates = coordinates[index]\n",
    "\n",
    "        assert 0 < len(atoms) <= self.__max_atoms\n",
    "\n",
    "        atom_vec = torch.from_numpy(self.dictionary.vec_index(atoms)).long()\n",
    "\n",
    "        if self.normalize_coords:\n",
    "            coordinates = coordinates - coordinates.mean(axis=0)\n",
    "\n",
    "        if self.add_special_token:\n",
    "            atom_vec = torch.cat([torch.LongTensor([self.bos]), atom_vec, torch.LongTensor([self.eos])])\n",
    "            coordinates = np.concatenate([np.zeros((1, 3)), coordinates, np.zeros((1, 3))], axis=0)\n",
    "\n",
    "        ## obtain edge types; which is defined as the combination of two atom types\n",
    "        edge_type = atom_vec.view(-1, 1) * self.num_types + atom_vec.view(1, -1)\n",
    "        dist = distance_matrix(coordinates, coordinates).astype(np.float32)\n",
    "        coordinates, dist = torch.from_numpy(coordinates), torch.from_numpy(dist)\n",
    "        return atom_vec, coordinates, edge_type, dist, smiles, description, enriched_description"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
